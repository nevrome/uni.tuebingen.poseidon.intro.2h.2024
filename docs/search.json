[
  {
    "objectID": "spacetime.html",
    "href": "spacetime.html",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "For data analysis and visualization an R-based workflow may be helpful. This is especially relevant if we want to incorporate spatial and temporal information.\n\n\nWe previously prepared the PCA dataset and we can load it again into an R environment.\n\npca_out &lt;- readRDS(\"scratch/pca_out.rds\")\n\nThis result dataset lacks all context information for the individual samples, which renders it really difficult to make sense of the PCA plot without prior knowledge. We do have plenty of context information, though, in the .janno file of the iceWithRefSmall package we prepared for the PCA in the first place. How do we access this?\nAs the .janno file is a simple .tsv file, we can easily read it into R, for example with readr::read_tsv() into a generic tibble.\nBeyond that the Poseidon ecosystem includes an R package janno to read .janno files into a more specific janno data structure with some additional features.\n\nref_janno &lt;- janno::read_janno(\"scratch/iceWithRefSmall\")\n\n\n\n\n\n\n\nNote\n\n\n\njanno::read_janno() applies a structural validity check to the input, transforms the columns to the correct data type for specified .janno columns and correctly detects list columns.\nAt the same time the janno S3 class is derived from tibble and thus fully interoperable with the tidyverse.\n\n\nWith this information ready we can easily enrich our PCA plot to make it more readable.\n\npca_with_context &lt;- dplyr::bind_cols(\n  pca_out$pca.sample_coordinates,\n  group = unlist(ref_janno$Group_Name)\n)\n\n\nlibrary(magrittr)\nlibrary(ggplot2)\n\np &lt;- pca_with_context %&gt;%\n  dplyr::mutate(group = dplyr::case_match(\n    group,\n    c(\"iceG\", \"English\", \"Syrian\", \"Sardinian\") ~ group,\n    .default = NA\n  )) %&gt;%\n  dplyr::arrange(!is.na(group), group) %&gt;%\n  ggplot() +\n  geom_point(\n    aes(x = PC1, y = PC2, colour = group)\n  ) +\n  scale_y_reverse() +\n  coord_fixed()\n\n\n\n\nPCA plot; the populations English, Sardinian, Syrian and iceG for the ice mummy are highlighted\n\n\n\n\n\nNow what about our own sample? As a first step we add the new information we received from Prof. P. to our Poseidon package ice. We do so by editing the .janno file at ice/ice.janno.\nStarting from\n\n\n\n\nPoseidon_ID\nGenetic_Sex\nGroup_Name\nLatitude\nLongitude\n\n\n\n\nice\nM\niceG\n46.77\n10.83\n\n\n\n\nwe add some additional columns for the age information.\n\n\n\n\n\n\n\n\n\n\n\nDate_Type\nDate_C14_Uncal_BP\nDate_C14_Uncal_BP_Err\nDate_BC_AD_Start\nDate_BC_AD_Stop\n\n\n\n\nC14\n4555\n34\nn/a\nn/a\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll pre-defined .janno file columns are specified as part of the package schema here. Beyond that the Poseidon websites features more detailed and concrete explanations of the columns here, for example for all columns related to sample age and dating: Temporal position.\n\n\nOptionally we can document this change to the package in a changelog, which we add to the package in the form of a CHANGELOG.md file.\n\n\nCHANGELOG.md\n\n- V 0.2.0: Added a radiocarbon date for the ice sample.\n\nWe should then also update the package version number in our POSEIDON.yml file and link the changelog there:\n\n\nPOSEIDON.yml\n\nposeidonVersion: 2.7.1\ntitle: ice\npackageVersion: 0.2.0\nlastModified: 2024-01-24\n...\nchangelogFile: CHANGELOG.md\n\nAfter implementing the change we should again check the structural validity of the package with trident validate.\n\ntrident validate -d ice\n\n\n\n\nWith the data added, we can load the .janno file for our sample into R.\n\nice_janno &lt;- janno::read_janno(\"scratch/ice\")\n\nAs the janno class is derived from tibble we can easily transform it to a spatial object as e.g. specified by the sf library. And we can then plot it together with arbitrary spatial reference data to create a (simple) map.\n\nice_sf &lt;- ice_janno %&gt;%\n  sf::st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\nworldwide_coastline &lt;- rnaturalearth::ne_coastline()\n\np &lt;- ggplot() +\n  geom_sf(data = worldwide_coastline) +\n  geom_sf(data = ice_sf, color = \"red\") +\n  coord_sf(xlim = c(-15,40), ylim = c(35, 60))\n\n\n\n\nMap of Western Eurasia; the ice mummy sampling location is marked with a red dot\n\n\nFor temporal information, the janno package includes a special convenience function: janno::process_age(). It can handle all kind of age data that can be specified for an individual sample in a Poseidon package.\nFor C14 date calibration it runs (sum)calibration through the Bchron R package1 to generate a post-calibration probability distribution, which in turn can be used for derived measures.\nprocess_age() appends up to five additional (list) columns to the janno file:\n\nice_age &lt;- ice_janno %&gt;% janno::process_age()\n\nThe Date_BC_AD_*_Derived columns simply include probable start, end and median ages:\n\nice_age$Date_BC_AD_Start_Derived\nice_age$Date_BC_AD_Median_Derived\nice_age$Date_BC_AD_Stop_Derived\n\nice_age$Date_BC_AD_Prob includes the full (post-calibration) year-wise probability distribution for a given sample.\nWe can plot the distribution similar to how for example OxCal would do it for us.\n\nprob &lt;- ice_age$Date_BC_AD_Prob[[1]]\n\np &lt;- ggplot() +\n  ggridges::geom_ridgeline(\n    data = prob,\n    mapping = aes(x = age, height = sum_dens, y = \"sample 1\"), scale = 30\n  ) +\n  geom_line(\n    data = prob %&gt;%\n      dplyr::mutate(\n        ts = cumsum(two_sigma != dplyr::lag(two_sigma, default = FALSE))\n      ) %&gt;%\n      dplyr::filter(two_sigma),\n    mapping = aes(x = age, y = \"sample 1\", group = ts),\n    position = position_nudge(y = -0.1)\n  ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\nRidge plot of the post-calibration probability distribution of the radiocarbon age; the 2-sigma sections are marked with a line plot below the curve\n\n\nRandom ages sampled from this distribution are available in ice_age$Date_BC_AD_Sample and may be even more important for further analyses.\n\n\n\n\n\n\n\nLearn more about…\n\n\n\n\nThe janno R package: janno R package\nThe Natural Earth Data project: Geodata download\nThe Bchron R package: Vignette: Calibrating radiocarbon dates"
  },
  {
    "objectID": "spacetime.html#working-with-.janno-context-information-in-r",
    "href": "spacetime.html#working-with-.janno-context-information-in-r",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "For data analysis and visualization an R-based workflow may be helpful. This is especially relevant if we want to incorporate spatial and temporal information.\n\n\nWe previously prepared the PCA dataset and we can load it again into an R environment.\n\npca_out &lt;- readRDS(\"scratch/pca_out.rds\")\n\nThis result dataset lacks all context information for the individual samples, which renders it really difficult to make sense of the PCA plot without prior knowledge. We do have plenty of context information, though, in the .janno file of the iceWithRefSmall package we prepared for the PCA in the first place. How do we access this?\nAs the .janno file is a simple .tsv file, we can easily read it into R, for example with readr::read_tsv() into a generic tibble.\nBeyond that the Poseidon ecosystem includes an R package janno to read .janno files into a more specific janno data structure with some additional features.\n\nref_janno &lt;- janno::read_janno(\"scratch/iceWithRefSmall\")\n\n\n\n\n\n\n\nNote\n\n\n\njanno::read_janno() applies a structural validity check to the input, transforms the columns to the correct data type for specified .janno columns and correctly detects list columns.\nAt the same time the janno S3 class is derived from tibble and thus fully interoperable with the tidyverse.\n\n\nWith this information ready we can easily enrich our PCA plot to make it more readable.\n\npca_with_context &lt;- dplyr::bind_cols(\n  pca_out$pca.sample_coordinates,\n  group = unlist(ref_janno$Group_Name)\n)\n\n\nlibrary(magrittr)\nlibrary(ggplot2)\n\np &lt;- pca_with_context %&gt;%\n  dplyr::mutate(group = dplyr::case_match(\n    group,\n    c(\"iceG\", \"English\", \"Syrian\", \"Sardinian\") ~ group,\n    .default = NA\n  )) %&gt;%\n  dplyr::arrange(!is.na(group), group) %&gt;%\n  ggplot() +\n  geom_point(\n    aes(x = PC1, y = PC2, colour = group)\n  ) +\n  scale_y_reverse() +\n  coord_fixed()\n\n\n\n\nPCA plot; the populations English, Sardinian, Syrian and iceG for the ice mummy are highlighted\n\n\n\n\n\nNow what about our own sample? As a first step we add the new information we received from Prof. P. to our Poseidon package ice. We do so by editing the .janno file at ice/ice.janno.\nStarting from\n\n\n\n\nPoseidon_ID\nGenetic_Sex\nGroup_Name\nLatitude\nLongitude\n\n\n\n\nice\nM\niceG\n46.77\n10.83\n\n\n\n\nwe add some additional columns for the age information.\n\n\n\n\n\n\n\n\n\n\n\nDate_Type\nDate_C14_Uncal_BP\nDate_C14_Uncal_BP_Err\nDate_BC_AD_Start\nDate_BC_AD_Stop\n\n\n\n\nC14\n4555\n34\nn/a\nn/a\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll pre-defined .janno file columns are specified as part of the package schema here. Beyond that the Poseidon websites features more detailed and concrete explanations of the columns here, for example for all columns related to sample age and dating: Temporal position.\n\n\nOptionally we can document this change to the package in a changelog, which we add to the package in the form of a CHANGELOG.md file.\n\n\nCHANGELOG.md\n\n- V 0.2.0: Added a radiocarbon date for the ice sample.\n\nWe should then also update the package version number in our POSEIDON.yml file and link the changelog there:\n\n\nPOSEIDON.yml\n\nposeidonVersion: 2.7.1\ntitle: ice\npackageVersion: 0.2.0\nlastModified: 2024-01-24\n...\nchangelogFile: CHANGELOG.md\n\nAfter implementing the change we should again check the structural validity of the package with trident validate.\n\ntrident validate -d ice\n\n\n\n\nWith the data added, we can load the .janno file for our sample into R.\n\nice_janno &lt;- janno::read_janno(\"scratch/ice\")\n\nAs the janno class is derived from tibble we can easily transform it to a spatial object as e.g. specified by the sf library. And we can then plot it together with arbitrary spatial reference data to create a (simple) map.\n\nice_sf &lt;- ice_janno %&gt;%\n  sf::st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\nworldwide_coastline &lt;- rnaturalearth::ne_coastline()\n\np &lt;- ggplot() +\n  geom_sf(data = worldwide_coastline) +\n  geom_sf(data = ice_sf, color = \"red\") +\n  coord_sf(xlim = c(-15,40), ylim = c(35, 60))\n\n\n\n\nMap of Western Eurasia; the ice mummy sampling location is marked with a red dot\n\n\nFor temporal information, the janno package includes a special convenience function: janno::process_age(). It can handle all kind of age data that can be specified for an individual sample in a Poseidon package.\nFor C14 date calibration it runs (sum)calibration through the Bchron R package1 to generate a post-calibration probability distribution, which in turn can be used for derived measures.\nprocess_age() appends up to five additional (list) columns to the janno file:\n\nice_age &lt;- ice_janno %&gt;% janno::process_age()\n\nThe Date_BC_AD_*_Derived columns simply include probable start, end and median ages:\n\nice_age$Date_BC_AD_Start_Derived\nice_age$Date_BC_AD_Median_Derived\nice_age$Date_BC_AD_Stop_Derived\n\nice_age$Date_BC_AD_Prob includes the full (post-calibration) year-wise probability distribution for a given sample.\nWe can plot the distribution similar to how for example OxCal would do it for us.\n\nprob &lt;- ice_age$Date_BC_AD_Prob[[1]]\n\np &lt;- ggplot() +\n  ggridges::geom_ridgeline(\n    data = prob,\n    mapping = aes(x = age, height = sum_dens, y = \"sample 1\"), scale = 30\n  ) +\n  geom_line(\n    data = prob %&gt;%\n      dplyr::mutate(\n        ts = cumsum(two_sigma != dplyr::lag(two_sigma, default = FALSE))\n      ) %&gt;%\n      dplyr::filter(two_sigma),\n    mapping = aes(x = age, y = \"sample 1\", group = ts),\n    position = position_nudge(y = -0.1)\n  ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\nRidge plot of the post-calibration probability distribution of the radiocarbon age; the 2-sigma sections are marked with a line plot below the curve\n\n\nRandom ages sampled from this distribution are available in ice_age$Date_BC_AD_Sample and may be even more important for further analyses.\n\n\n\n\n\n\n\nLearn more about…\n\n\n\n\nThe janno R package: janno R package\nThe Natural Earth Data project: Geodata download\nThe Bchron R package: Vignette: Calibrating radiocarbon dates"
  },
  {
    "objectID": "spacetime.html#footnotes",
    "href": "spacetime.html#footnotes",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHaslett and Parnell (2008)↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A short introduction to the Poseidon genotype data management framework",
    "section": "",
    "text": "This website was created to support a demonstration of Poseidon and some of its tools, features and components for students and other practitioners of archaeogenetics. Read more about Poseidon at https://www.poseidon-adna.org/.\nThe introduction begins with the overview slides below, then shows some concrete applications and workflows in the Demo section, motivated by a little narrative framework.\n\nThis course was prepared and presented by Clemens Schmid.\nIt relies on the following Poseidon standard and software versions:\n\nPoseidon schema v2.7.1\ntrident v1.4.0.3\nqjanno v1.0.0.0\njanno R package v1.0.0\nxerxes v1.0.0.1"
  },
  {
    "objectID": "data_pub.html",
    "href": "data_pub.html",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "So far we have used data in the public Poseidon community archive, now it’s time to learn how to give back. Poseidon is very dependent on data and time donations from its community.\n\n\nIn a first step we have to make ourselves familiar with a a number of core technologies.\n\nCreating and validating Poseidon packages with the trident tool.\nFree and open source distributed version control with Git.\nCollaborative working on Git projects with GitHub.\nHandling large files in Git using Git LFS.\n\nWe have to install all of these software tools and set them up correctly. This is less daunting than it sounds, because: Superficial knowledge is sufficient and knowing them is useful far beyond this particular task.\n\n\n\nWe already built a minimal Poseidon package ice for our data in previous sections. We even made modifications to its .janno file to include more context data.\nBefore we submit the package we should complete the information in POSEIDON.yml, .janno, .bib (and maybe .ssf), so include all meta data we consider relevant.\n\n\n\n\n\n\nNote\n\n\n\nAll of these files are well specified and documented. The Poseidon package standard defines their general structure and various sub-pages on the website explain the individual variables in more detail: Genotype data details, .janno file details, .ssf file details\n\n\nHere is the current state of our POSEIDON.yml file:\n\n\n\nPOSEIDON.yml (ice package)\n\nposeidonVersion: 2.7.1\ntitle: ice\npackageVersion: 0.2.0\nlastModified: 2024-01-24\ngenotypeData:\n  format: EIGENSTRAT\n  genoFile: ice.geno\n  snpFile: ice.snp\n  indFile: ice.ind\n  snpSet: 1240K\njannoFile: ice.janno\nchangelogFile: CHANGELOG.md\n\n\nAnd here is a POSEIDON.yml file from a recently submitted, real world package for Peltola et al. (2023):\n\n\n\nPOSEIDON.yml (2023_Peltola_VolgaOka package)\n\nposeidonVersion: 2.7.1\ntitle: 2023_Peltola_VolgaOka\ndescription: Genetic admixture and language shift in the medieval Volga-Oka interfluve\ncontributor:\n- name: Sanni Peltola\n  email: sanni.peltola@helsinki.fi\npackageVersion: 1.0.0\nlastModified: 2022-12-15\ngenotypeData:\n  format: PLINK\n  genoFile: 2023_Peltola_VolgaOka.bed\n  genoFileChkSum: 2ab24d1c0f5cf7946d5a2f9dc115d2a2\n  snpFile: 2023_Peltola_VolgaOka.bim\n  snpFileChkSum: 50f8b2a49d819a3ee13e79f06bd70ff1\n  indFile: 2023_Peltola_VolgaOka.fam\n  indFileChkSum: 69c3ce7fb648c10adcae4f8d10cbafb4\n  snpSet: 1240K\njannoFile: 2023_Peltola_VolgaOka.janno\njannoFileChkSum: 9b2c20479d92bebca6e627479f475dda\nbibFile: 2023_Peltola_VolgaOka.bib\nbibFileChkSum: 2262b38bb4e5d8bfff2353fdbcb5bd20\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome tasks in the package preparation process for these files can be automated with trident rectify, for example adding the checksums for each package file.\n\n\nWhen we applied all necessary modifications we can confirm that the package is still valid with trident validate -d ice.\n\n\n\nTo submit the final package version we have to create a fork of the community archive repository on GitHub. This requires a GitHub account.\n\n\n\nPress the fork button in the top right corner to fork a repository on GitHub\n\n\nAnd then clone the fork to our computer, while omitting the large genotype data files. Note that this requires several setup steps to work correctly:\n\nGit has be installed for your computer (see here)\nYou must have created an ssh key pair to connect to GitHub via ssh (see here)\nGit LFS has to be installed (see here) and and configured for your user with git lfs install\n\nGIT_LFS_SKIP_SMUDGE=1 git clone git@github.com:&lt;yourGitHubUserName&gt;/community-archive.git\nWith the cloned repository on our system we can copy the files into the repositories directory and commit the changes.\n\n\n\nin the community-archive directory\n\ncp -r ../ice ice\ngit add ice\ngit commit -m \"added a first draft of the ice package\"\ngit push\n\n\nIn a last step we can open a Pull Request on GitHub from our fork to the original archive repository. Poseidon core members will take it from here.\n\n\n\nWhen you pushed to your fork, GitHub will automatically offer to “contribute” to the source repository\n\n\n\n\n\n\n\n\nLearn more about…\n\n\n\n\nThe submission process for the community archive: Submission guide"
  },
  {
    "objectID": "data_pub.html#submitting-a-package-to-the-poseidon-community-archive",
    "href": "data_pub.html#submitting-a-package-to-the-poseidon-community-archive",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "So far we have used data in the public Poseidon community archive, now it’s time to learn how to give back. Poseidon is very dependent on data and time donations from its community.\n\n\nIn a first step we have to make ourselves familiar with a a number of core technologies.\n\nCreating and validating Poseidon packages with the trident tool.\nFree and open source distributed version control with Git.\nCollaborative working on Git projects with GitHub.\nHandling large files in Git using Git LFS.\n\nWe have to install all of these software tools and set them up correctly. This is less daunting than it sounds, because: Superficial knowledge is sufficient and knowing them is useful far beyond this particular task.\n\n\n\nWe already built a minimal Poseidon package ice for our data in previous sections. We even made modifications to its .janno file to include more context data.\nBefore we submit the package we should complete the information in POSEIDON.yml, .janno, .bib (and maybe .ssf), so include all meta data we consider relevant.\n\n\n\n\n\n\nNote\n\n\n\nAll of these files are well specified and documented. The Poseidon package standard defines their general structure and various sub-pages on the website explain the individual variables in more detail: Genotype data details, .janno file details, .ssf file details\n\n\nHere is the current state of our POSEIDON.yml file:\n\n\n\nPOSEIDON.yml (ice package)\n\nposeidonVersion: 2.7.1\ntitle: ice\npackageVersion: 0.2.0\nlastModified: 2024-01-24\ngenotypeData:\n  format: EIGENSTRAT\n  genoFile: ice.geno\n  snpFile: ice.snp\n  indFile: ice.ind\n  snpSet: 1240K\njannoFile: ice.janno\nchangelogFile: CHANGELOG.md\n\n\nAnd here is a POSEIDON.yml file from a recently submitted, real world package for Peltola et al. (2023):\n\n\n\nPOSEIDON.yml (2023_Peltola_VolgaOka package)\n\nposeidonVersion: 2.7.1\ntitle: 2023_Peltola_VolgaOka\ndescription: Genetic admixture and language shift in the medieval Volga-Oka interfluve\ncontributor:\n- name: Sanni Peltola\n  email: sanni.peltola@helsinki.fi\npackageVersion: 1.0.0\nlastModified: 2022-12-15\ngenotypeData:\n  format: PLINK\n  genoFile: 2023_Peltola_VolgaOka.bed\n  genoFileChkSum: 2ab24d1c0f5cf7946d5a2f9dc115d2a2\n  snpFile: 2023_Peltola_VolgaOka.bim\n  snpFileChkSum: 50f8b2a49d819a3ee13e79f06bd70ff1\n  indFile: 2023_Peltola_VolgaOka.fam\n  indFileChkSum: 69c3ce7fb648c10adcae4f8d10cbafb4\n  snpSet: 1240K\njannoFile: 2023_Peltola_VolgaOka.janno\njannoFileChkSum: 9b2c20479d92bebca6e627479f475dda\nbibFile: 2023_Peltola_VolgaOka.bib\nbibFileChkSum: 2262b38bb4e5d8bfff2353fdbcb5bd20\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome tasks in the package preparation process for these files can be automated with trident rectify, for example adding the checksums for each package file.\n\n\nWhen we applied all necessary modifications we can confirm that the package is still valid with trident validate -d ice.\n\n\n\nTo submit the final package version we have to create a fork of the community archive repository on GitHub. This requires a GitHub account.\n\n\n\nPress the fork button in the top right corner to fork a repository on GitHub\n\n\nAnd then clone the fork to our computer, while omitting the large genotype data files. Note that this requires several setup steps to work correctly:\n\nGit has be installed for your computer (see here)\nYou must have created an ssh key pair to connect to GitHub via ssh (see here)\nGit LFS has to be installed (see here) and and configured for your user with git lfs install\n\nGIT_LFS_SKIP_SMUDGE=1 git clone git@github.com:&lt;yourGitHubUserName&gt;/community-archive.git\nWith the cloned repository on our system we can copy the files into the repositories directory and commit the changes.\n\n\n\nin the community-archive directory\n\ncp -r ../ice ice\ngit add ice\ngit commit -m \"added a first draft of the ice package\"\ngit push\n\n\nIn a last step we can open a Pull Request on GitHub from our fork to the original archive repository. Poseidon core members will take it from here.\n\n\n\nWhen you pushed to your fork, GitHub will automatically offer to “contribute” to the source repository\n\n\n\n\n\n\n\n\nLearn more about…\n\n\n\n\nThe submission process for the community archive: Submission guide"
  },
  {
    "objectID": "backstory3.html",
    "href": "backstory3.html",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "Many months have passed. You prepared, submitted and resubmitted an excellent manuscript about the ice mummy. The journal just wrote you that the paper will be published on this very day.\nIn this glorious moment you see another email from Prof. P.:\n\nMy dear student,\nThere is just one last thing to do: We have to share the genotype data, so that the community can reproduce our analysis!\nCan you please submit our package to the Poseidon community archive?\nBest, CP\n\n\n\n\n\n\nAs evident from another photograph1, Prof. P. is already celebrating.\nWell - one last step to complete this project! Fortunately the package is almost publication-ready, after you worked with it extensively for the last months. But how does this submission work?"
  },
  {
    "objectID": "backstory3.html#concluding-another-project",
    "href": "backstory3.html#concluding-another-project",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "Many months have passed. You prepared, submitted and resubmitted an excellent manuscript about the ice mummy. The journal just wrote you that the paper will be published on this very day.\nIn this glorious moment you see another email from Prof. P.:\n\nMy dear student,\nThere is just one last thing to do: We have to share the genotype data, so that the community can reproduce our analysis!\nCan you please submit our package to the Poseidon community archive?\nBest, CP\n\n\n\n\n\n\nAs evident from another photograph1, Prof. P. is already celebrating.\nWell - one last step to complete this project! Fortunately the package is almost publication-ready, after you worked with it extensively for the last months. But how does this submission work?"
  },
  {
    "objectID": "backstory3.html#footnotes",
    "href": "backstory3.html#footnotes",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPrompt : “A film still of yetis and mountain climbers celebrating a scientific paper in a mountain setting. The atmosphere is bright and cheerful.” Made with Microsoft Bing Image Creator.↩︎"
  },
  {
    "objectID": "backstory1.html",
    "href": "backstory1.html",
    "title": "Hunt for the Ice mummy",
    "section": "",
    "text": "It is the year 2024 in an alternative universe. You are a student of archaeogenetics at Tübingen university. One cold February afternoon you receive a cryptic email from your professor. You have not seen him for months, when he had left for an expedition in the Tyrolean Alps.\n\nMy dear student,\nwe made an incredible discovery at 46.77° N, 10.83° E! You have to look at this data immediately. I will contact you, when I know more.\nBest, CP\n\nAttached you find multiple files. First this incredible photograph1:\n\n\n\n\n\nBut then also genotype data2 files in EIGENSTRAT format:\n\n\nice.ind\n\nice M   iceG\n\n\n\nice.snp\n\nrs3094315   1   2.013e-2    752566  G   A\nrs12124819  1   2.0242e-2   776546  G   A\nrs28765502  1   2.2137e-2   832918  C   T\nrs7419119   1   2.2518e-2   842013  G   T\nrs950122    1   2.272e-2    846864  C   G\n...\n\n\n\nice.geno\n\n0\n0\n2\n2\n0\n...\n\nThey seem to feature the SNPs for a single individual. With few missing values, certainly enough to work with.\nYou are justifiably concerned about the ethical implications of working with a sample you know so little about, but you put these concerns aside for the moment. You want to get to the bottom of this mystery first."
  },
  {
    "objectID": "backstory1.html#footnotes",
    "href": "backstory1.html#footnotes",
    "title": "Hunt for the Ice mummy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “photograph” is AI-generated with the following prompt: “A film still of a famous German mountain climber and the Yeti discovering an Ice mummy in the Tyrolean Alps”. Made with Microsoft Bing Image Creator.↩︎\nThe genotype data was taken from Keller et al. (2012). See iceman_data_prep.sh for a script to prepare the data in the minimal version assumed here.↩︎"
  },
  {
    "objectID": "backstory2.html",
    "href": "backstory2.html",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "Just while you still ogle at the first PCA result you receive another email from Prof. P. with a second photograph1 from the field lab.\n\n\n\n\n\n\nMy dear student,\nwe made tremendous progress with our preliminary analysis! Here’s the radiocarbon age we measured from the mummy:\n4555 ± 34 BP\nPlease calibrate this date for me. Oh - and make a map based on the coordinates I sent you.\nBest, CP\n\nThat sounds urgent! You get to work right away."
  },
  {
    "objectID": "backstory2.html#another-message-from-prof.-p.",
    "href": "backstory2.html#another-message-from-prof.-p.",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "Just while you still ogle at the first PCA result you receive another email from Prof. P. with a second photograph1 from the field lab.\n\n\n\n\n\n\nMy dear student,\nwe made tremendous progress with our preliminary analysis! Here’s the radiocarbon age we measured from the mummy:\n4555 ± 34 BP\nPlease calibrate this date for me. Oh - and make a map based on the coordinates I sent you.\nBest, CP\n\nThat sounds urgent! You get to work right away."
  },
  {
    "objectID": "backstory2.html#footnotes",
    "href": "backstory2.html#footnotes",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPrompt : “A film still of the frozen hand of an ice mummy in a mass spectrometer. In the background the yeti works on the massive machine”. Made with Microsoft Bing Image Creator.↩︎"
  },
  {
    "objectID": "data_prep.html",
    "href": "data_prep.html",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "As a first step of our analysis we should lay some groundwork and transform our research data into a tidy and interoperable data structure.\n\n\nWe’re going to need some scratch space for this analysis, so we create a new directory scratch.\n\nmkdir scratch\ncd scratch\n\nWe then copy the data we received from Prof. P. into a directory scratch/iceGeno.\n\n\n\nAs we want to use Poseidon for this analysis, we have to install its main software tool: trident. On Linux we can simply install it on the command line.\n\n# download the current stable release binary\nwget https://github.com/poseidon-framework/poseidon-hs/releases/latest/download/trident-Linux\n# rename it to simply trident\nmv trident-Linux trident\n# make it executable\nchmod +x trident\n# run it to test if it is working\n./trident -h\n\nWith trident ready and running we can turn to the data.\n\n\n\n\n\n\nNote\n\n\n\ntrident is also available for Windows and macOS, but security features of these operating systems complicate the installation. If you trust trident then you can turn these protections off and run the executable just as on Linux.\n\n\n\n\n\nAt the moment we only have the Eigenstrat data as provided by Prof. P.. To incorporate context information, for example the coordinates we received from him, we can create a Poseidon package around it.\n\n\n\n\n\n\nNote\n\n\n\nThis step is not absolutely necessary to use trident. trident supports direct handling of genotype data without a Poseidon package with the -p/--genoOne flag for unpackaged in- and the --onlyGeno flag for unpackaged output. You can still profit from trident’s convenient subsetting and merging abilities, even if you do not want to use any other Poseidon features.\n\n\nWe create the package with trident’s init subcommand, which wraps genotype data in a Poseidon package.\nBy running\n\n./trident init \\\n  -p iceGeno/ice.geno \\\n  -o ice\n\n\n\nwe turn\niceGeno\n├── ice.geno\n├── ice.ind\n└── ice.snp\n\ninto\nice\n├── ice.bib\n├── ice.geno\n├── ice.ind\n├── ice.janno\n├── ice.snp\n└── POSEIDON.yml\n\n\nThe POSEIDON.yml defines the package. init returns a technically valid template, which we can simplify for now.\n\n\nPOSEIDON.yml (automatically generated)\n\nposeidonVersion: 2.7.1\ntitle: ice\ndescription: Empty package template. Please add a description\ncontributor:\n- name: Josiah Carberry\n  email: carberry@brown.edu\n  orcid: 0000-0002-1825-0097\npackageVersion: 0.1.0\nlastModified: 2024-01-24\ngenotypeData:\n  format: EIGENSTRAT\n  genoFile: ice.geno\n  snpFile: ice.snp\n  indFile: ice.ind\n  snpSet: Other\njannoFile: ice.janno\nbibFile: ice.bib\n\n\n\nPOSEIDON.yml (simplified)\n\nposeidonVersion: 2.7.1\ntitle: ice\npackageVersion: 0.1.0\nlastModified: 2024-01-24\ngenotypeData:\n  format: EIGENSTRAT\n  genoFile: ice.geno\n  snpFile: ice.snp\n  indFile: ice.ind\n  snpSet: 1240K\njannoFile: ice.janno\n\nWe can delete the .bib file, as we have no literature references for our new sample yet.\nThe generated .janno file features a lot of empty columns. We can delete them and only keep and fill the ones for which we have information: Latitude and Longitude.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoseidon_ID\nGenetic_Sex\nGroup_Name\nAlternative_IDs\nRelation_To\nRelation_Degree\nRelation_Type\nRelation_Note\nCollection_ID\nCountry\nCountry_ISO\nLocation\nSite\nLatitude\nLongitude\nDate_Type\nDate_C14_Labnr\nDate_C14_Uncal_BP\nDate_C14_Uncal_BP_Err\nDate_BC_AD_Start\nDate_BC_AD_Median\nDate_BC_AD_Stop\nDate_Note\nMT_Haplogroup\nY_Haplogroup\nSource_Tissue\nNr_Libraries\nLibrary_Names\nCapture_Type\nUDG\nLibrary_Built\nGenotype_Ploidy\nData_Preparation_Pipeline_URL\nEndogenous\nNr_SNPs\nCoverage_on_Target_SNPs\nDamage\nContamination\nContamination_Err\nContamination_Meas\nContamination_Note\nGenetic_Source_Accession_IDs\nPrimary_Contact\nPublication\nNote\nKeywords\n\n\n\n\nice\nM\niceG\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\n\n\n\n\n\n\n\n\nPoseidon_ID\nGenetic_Sex\nGroup_Name\nLatitude\nLongitude\n\n\n\n\nice\nM\niceG\n46.77\n10.83\n\n\n\n\n\n\n\nAfter all these manual changes we should confirm that the package is still structurally valid and computer-readable.\ntrident has a special subcommand for that: validate.\n\n./trident validate -d ice\n\nIf we did everything right, then the validation should pass. Otherwise we should go back to the package an fix it. trident only reads valid packages. If we encounter an issue we may get more information about it by running trident with the --debug flag.\n\n\n\n\n\n\nNote\n\n\n\nvalidate supports not just the validation of whole packages, but also of individual .janno, .geno or .bib files. It can also ignore the genotype data in a package with --ignoreGeno, or parse the whole genotype data files with --fullGeno. As a default it attempts to parse the first 100 SNPs.\n\n\n\n\n\n\n\n\n\nLearn more about…\n\n\n\n\nThe structure of the Poseidon package: The Poseidon package\nThe trident software tool: trident CLI software"
  },
  {
    "objectID": "data_prep.html#preparing-a-poseidon-package",
    "href": "data_prep.html#preparing-a-poseidon-package",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "As a first step of our analysis we should lay some groundwork and transform our research data into a tidy and interoperable data structure.\n\n\nWe’re going to need some scratch space for this analysis, so we create a new directory scratch.\n\nmkdir scratch\ncd scratch\n\nWe then copy the data we received from Prof. P. into a directory scratch/iceGeno.\n\n\n\nAs we want to use Poseidon for this analysis, we have to install its main software tool: trident. On Linux we can simply install it on the command line.\n\n# download the current stable release binary\nwget https://github.com/poseidon-framework/poseidon-hs/releases/latest/download/trident-Linux\n# rename it to simply trident\nmv trident-Linux trident\n# make it executable\nchmod +x trident\n# run it to test if it is working\n./trident -h\n\nWith trident ready and running we can turn to the data.\n\n\n\n\n\n\nNote\n\n\n\ntrident is also available for Windows and macOS, but security features of these operating systems complicate the installation. If you trust trident then you can turn these protections off and run the executable just as on Linux.\n\n\n\n\n\nAt the moment we only have the Eigenstrat data as provided by Prof. P.. To incorporate context information, for example the coordinates we received from him, we can create a Poseidon package around it.\n\n\n\n\n\n\nNote\n\n\n\nThis step is not absolutely necessary to use trident. trident supports direct handling of genotype data without a Poseidon package with the -p/--genoOne flag for unpackaged in- and the --onlyGeno flag for unpackaged output. You can still profit from trident’s convenient subsetting and merging abilities, even if you do not want to use any other Poseidon features.\n\n\nWe create the package with trident’s init subcommand, which wraps genotype data in a Poseidon package.\nBy running\n\n./trident init \\\n  -p iceGeno/ice.geno \\\n  -o ice\n\n\n\nwe turn\niceGeno\n├── ice.geno\n├── ice.ind\n└── ice.snp\n\ninto\nice\n├── ice.bib\n├── ice.geno\n├── ice.ind\n├── ice.janno\n├── ice.snp\n└── POSEIDON.yml\n\n\nThe POSEIDON.yml defines the package. init returns a technically valid template, which we can simplify for now.\n\n\nPOSEIDON.yml (automatically generated)\n\nposeidonVersion: 2.7.1\ntitle: ice\ndescription: Empty package template. Please add a description\ncontributor:\n- name: Josiah Carberry\n  email: carberry@brown.edu\n  orcid: 0000-0002-1825-0097\npackageVersion: 0.1.0\nlastModified: 2024-01-24\ngenotypeData:\n  format: EIGENSTRAT\n  genoFile: ice.geno\n  snpFile: ice.snp\n  indFile: ice.ind\n  snpSet: Other\njannoFile: ice.janno\nbibFile: ice.bib\n\n\n\nPOSEIDON.yml (simplified)\n\nposeidonVersion: 2.7.1\ntitle: ice\npackageVersion: 0.1.0\nlastModified: 2024-01-24\ngenotypeData:\n  format: EIGENSTRAT\n  genoFile: ice.geno\n  snpFile: ice.snp\n  indFile: ice.ind\n  snpSet: 1240K\njannoFile: ice.janno\n\nWe can delete the .bib file, as we have no literature references for our new sample yet.\nThe generated .janno file features a lot of empty columns. We can delete them and only keep and fill the ones for which we have information: Latitude and Longitude.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoseidon_ID\nGenetic_Sex\nGroup_Name\nAlternative_IDs\nRelation_To\nRelation_Degree\nRelation_Type\nRelation_Note\nCollection_ID\nCountry\nCountry_ISO\nLocation\nSite\nLatitude\nLongitude\nDate_Type\nDate_C14_Labnr\nDate_C14_Uncal_BP\nDate_C14_Uncal_BP_Err\nDate_BC_AD_Start\nDate_BC_AD_Median\nDate_BC_AD_Stop\nDate_Note\nMT_Haplogroup\nY_Haplogroup\nSource_Tissue\nNr_Libraries\nLibrary_Names\nCapture_Type\nUDG\nLibrary_Built\nGenotype_Ploidy\nData_Preparation_Pipeline_URL\nEndogenous\nNr_SNPs\nCoverage_on_Target_SNPs\nDamage\nContamination\nContamination_Err\nContamination_Meas\nContamination_Note\nGenetic_Source_Accession_IDs\nPrimary_Contact\nPublication\nNote\nKeywords\n\n\n\n\nice\nM\niceG\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\n\n\n\n\n\n\n\n\nPoseidon_ID\nGenetic_Sex\nGroup_Name\nLatitude\nLongitude\n\n\n\n\nice\nM\niceG\n46.77\n10.83\n\n\n\n\n\n\n\nAfter all these manual changes we should confirm that the package is still structurally valid and computer-readable.\ntrident has a special subcommand for that: validate.\n\n./trident validate -d ice\n\nIf we did everything right, then the validation should pass. Otherwise we should go back to the package an fix it. trident only reads valid packages. If we encounter an issue we may get more information about it by running trident with the --debug flag.\n\n\n\n\n\n\nNote\n\n\n\nvalidate supports not just the validation of whole packages, but also of individual .janno, .geno or .bib files. It can also ignore the genotype data in a package with --ignoreGeno, or parse the whole genotype data files with --fullGeno. As a default it attempts to parse the first 100 SNPs.\n\n\n\n\n\n\n\n\n\nLearn more about…\n\n\n\n\nThe structure of the Poseidon package: The Poseidon package\nThe trident software tool: trident CLI software"
  },
  {
    "objectID": "fstats.html",
    "href": "fstats.html",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "Understanding sample positions in PCA-, spatial- and temporal space are fundamentals of any archaeogenetic research project. But so are also various other methods for data exploration and statistical inference. For F-statistics Poseidon features the dedicated xerxes software tool.\n\n\nWe can install xerxes just as trident and qjanno (there is no windows version, though):\n\n# download the current stable release binary\nwget https://github.com/poseidon-framework/poseidon-analysis-hs/releases/latest/download/xerxes-Linux\n# rename it to simply xerxes\nmv xerxes-Linux xerxes\n# make it executable\nchmod +x xerxes\n# run it to test if it is working\n./xerxes -h\n\nxerxes also supports some other experimental features, but only its implementation of F-statistics is stable right now.\n\n\n\nFor the ice mummy we previously concluded from the PCA plot, that it seems to feature an ancestry profile not unlike the of what we can observe for present day Sardinians. It plots far closer to them than for example to modern-day English reference genomes. xerxes allows us to test this specific comparison with a test encoded as F4(&lt;Chimp.REF&gt;,&lt;ice&gt;,Sardinian,French) vs. F4(&lt;Chimp.REF&gt;,&lt;ice&gt;,English,French), so \\(\\textrm{F4}(\\textrm{Chimp},\\textrm{Ice mummy};\\textrm{Sardinian},\\textrm{French})\\) vs. \\(\\textrm{F4}(\\textrm{Chimp},\\textrm{Ice mummy};\\textrm{English},\\textrm{French})\\).\n\n# download the Reference_Genomes package for the Chimp genome\ntrident fetch -d refData -f \"*Reference_Genomes*\"\n# run the two F4 tests\nxerxes fstats -d refData -d ice \\\n  --stat 'F4(&lt;Chimp.REF&gt;,&lt;ice&gt;,Sardinian,French)' \\\n  --stat 'F4(&lt;Chimp.REF&gt;,&lt;ice&gt;,English,French)'\n\n\n\n\n\n\n\nNote\n\n\n\nxerxes uses stream-processing to read samples from different Poseidon packages simultaneously and with small memory footprint.\n\n\nThis command line interface is suitable for a small set of targeted tests. For bigger test series where we want to explore large sets of permutations among the individuals and populations of interest xerxes features a powerful .yml based configuration file format.\n\nfstats:\n- type: F4\n  a: [\"&lt;Chimp.REF&gt;\"]\n  b: [\"&lt;Iceman.SG&gt;\"]\n  c: [\"English\", \"Sardinian\"]\n  d: [\"French\"]\n\n\nxerxes fstats -d ref_data \\\n  --statConfig f4.config\n\n\n\n\n\n\n\nNote\n\n\n\nThis config file format allows to define groups dynamically with a simple selection syntax. This includes both positive and negative selection\n\ngroupDefs:\n  FR: [\"French\", \"French.SDG\", \"-&lt;French23812&gt;\", \"-&lt;French23830&gt;\"]\n  EN: [\"English\", \"-&lt;HG00131&gt;\"]\nfstats:\n- type: F4\n  a: [\"&lt;Chimp.REF&gt;\"]\n  b: [\"&lt;Iceman.SG&gt;\"]\n  c: [\"EN\", \"Sardinian\"]\n  d: [\"FR\"]\n\nIt also allow to list multiple different statistics in the fstats block, which will then all be calculated in one xerxes run.\n\n\nThe F4 test shown here is not particular interesting though - we typically want to dive much deeper and beyond modern reference genomes. One relevant question for for an ancient sample from Western Eurasia may be, to which major ancestry components it has the strongest affinities: Western Hunter-Gatherer ancestry, Ancient Near-Eastern ancestry or Steppe ancestry.\nEach of these components is represented by multiple ancient reference genomes, each in turn grouped into populations by their shared origin in space, time and by the archaeological material they are associated with. Here we run one possible outgroup-F3 test with three individual reference populations1.\n\n# download the various packages that include samples for the desired\n# reference genomes\ntrident fetch -d refData \\ \n  -f \"Austria_EN_LBK, Croatia_Mesolithic_HG, Russia_Samara_EBA_Yamnaya, Mbuti.DG\"\n# run three F3 tests\nxerxes fstats -d refData -d ice \\\n  --stat 'F3(&lt;ice&gt;,Austria_EN_LBK,Mbuti.DG)' \\\n  --stat 'F3(&lt;ice&gt;,Croatia_Mesolithic_HG,Mbuti.DG)' \\\n  --stat 'F3(&lt;ice&gt;,Russia_Samara_EBA_Yamnaya,Mbuti.DG)' \\\n  -f outgroupF3.tsv\n\nHere we wrote the results to an output file, which we can read into R to visualize the relevant estimates.\n\nlibrary(magrittr)\nlibrary(ggplot2)\n\noutgroup_f3 &lt;- readr::read_tsv(\"scratch/outgroupF3.tsv\")\np &lt;- outgroup_f3 %&gt;%\n  ggplot() +\n  geom_point(aes(x = b, y = Estimate_Jackknife)) +\n  geom_errorbar(\n    aes(\n      x = b,\n      ymin = Estimate_Jackknife - StdErr_Jackknife,\n      ymax = Estimate_Jackknife + StdErr_Jackknife\n    ), width = 0.2\n  ) +\n  coord_flip() +\n  scale_x_discrete(limits = rev) +\n  theme(axis.title.y = element_blank())\n\n\n\n\nF-stats plot; estimated values and errors for each test population on the y-axis\n\n\n\n\n\n\n\n\nLearn more about…\n\n\n\n\nThe trident fetch -&gt; xerxes fstats workflow: Xerxes’s cutlery - enjoying your dinner?\nF-statistics and how to use xerxes: Introduction to F3- and F4-Statistics\nxerxes’ algorithms and implementation details: xerxes Whitepaper"
  },
  {
    "objectID": "fstats.html#f-statistics-with-poseidon-packages",
    "href": "fstats.html#f-statistics-with-poseidon-packages",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "Understanding sample positions in PCA-, spatial- and temporal space are fundamentals of any archaeogenetic research project. But so are also various other methods for data exploration and statistical inference. For F-statistics Poseidon features the dedicated xerxes software tool.\n\n\nWe can install xerxes just as trident and qjanno (there is no windows version, though):\n\n# download the current stable release binary\nwget https://github.com/poseidon-framework/poseidon-analysis-hs/releases/latest/download/xerxes-Linux\n# rename it to simply xerxes\nmv xerxes-Linux xerxes\n# make it executable\nchmod +x xerxes\n# run it to test if it is working\n./xerxes -h\n\nxerxes also supports some other experimental features, but only its implementation of F-statistics is stable right now.\n\n\n\nFor the ice mummy we previously concluded from the PCA plot, that it seems to feature an ancestry profile not unlike the of what we can observe for present day Sardinians. It plots far closer to them than for example to modern-day English reference genomes. xerxes allows us to test this specific comparison with a test encoded as F4(&lt;Chimp.REF&gt;,&lt;ice&gt;,Sardinian,French) vs. F4(&lt;Chimp.REF&gt;,&lt;ice&gt;,English,French), so \\(\\textrm{F4}(\\textrm{Chimp},\\textrm{Ice mummy};\\textrm{Sardinian},\\textrm{French})\\) vs. \\(\\textrm{F4}(\\textrm{Chimp},\\textrm{Ice mummy};\\textrm{English},\\textrm{French})\\).\n\n# download the Reference_Genomes package for the Chimp genome\ntrident fetch -d refData -f \"*Reference_Genomes*\"\n# run the two F4 tests\nxerxes fstats -d refData -d ice \\\n  --stat 'F4(&lt;Chimp.REF&gt;,&lt;ice&gt;,Sardinian,French)' \\\n  --stat 'F4(&lt;Chimp.REF&gt;,&lt;ice&gt;,English,French)'\n\n\n\n\n\n\n\nNote\n\n\n\nxerxes uses stream-processing to read samples from different Poseidon packages simultaneously and with small memory footprint.\n\n\nThis command line interface is suitable for a small set of targeted tests. For bigger test series where we want to explore large sets of permutations among the individuals and populations of interest xerxes features a powerful .yml based configuration file format.\n\nfstats:\n- type: F4\n  a: [\"&lt;Chimp.REF&gt;\"]\n  b: [\"&lt;Iceman.SG&gt;\"]\n  c: [\"English\", \"Sardinian\"]\n  d: [\"French\"]\n\n\nxerxes fstats -d ref_data \\\n  --statConfig f4.config\n\n\n\n\n\n\n\nNote\n\n\n\nThis config file format allows to define groups dynamically with a simple selection syntax. This includes both positive and negative selection\n\ngroupDefs:\n  FR: [\"French\", \"French.SDG\", \"-&lt;French23812&gt;\", \"-&lt;French23830&gt;\"]\n  EN: [\"English\", \"-&lt;HG00131&gt;\"]\nfstats:\n- type: F4\n  a: [\"&lt;Chimp.REF&gt;\"]\n  b: [\"&lt;Iceman.SG&gt;\"]\n  c: [\"EN\", \"Sardinian\"]\n  d: [\"FR\"]\n\nIt also allow to list multiple different statistics in the fstats block, which will then all be calculated in one xerxes run.\n\n\nThe F4 test shown here is not particular interesting though - we typically want to dive much deeper and beyond modern reference genomes. One relevant question for for an ancient sample from Western Eurasia may be, to which major ancestry components it has the strongest affinities: Western Hunter-Gatherer ancestry, Ancient Near-Eastern ancestry or Steppe ancestry.\nEach of these components is represented by multiple ancient reference genomes, each in turn grouped into populations by their shared origin in space, time and by the archaeological material they are associated with. Here we run one possible outgroup-F3 test with three individual reference populations1.\n\n# download the various packages that include samples for the desired\n# reference genomes\ntrident fetch -d refData \\ \n  -f \"Austria_EN_LBK, Croatia_Mesolithic_HG, Russia_Samara_EBA_Yamnaya, Mbuti.DG\"\n# run three F3 tests\nxerxes fstats -d refData -d ice \\\n  --stat 'F3(&lt;ice&gt;,Austria_EN_LBK,Mbuti.DG)' \\\n  --stat 'F3(&lt;ice&gt;,Croatia_Mesolithic_HG,Mbuti.DG)' \\\n  --stat 'F3(&lt;ice&gt;,Russia_Samara_EBA_Yamnaya,Mbuti.DG)' \\\n  -f outgroupF3.tsv\n\nHere we wrote the results to an output file, which we can read into R to visualize the relevant estimates.\n\nlibrary(magrittr)\nlibrary(ggplot2)\n\noutgroup_f3 &lt;- readr::read_tsv(\"scratch/outgroupF3.tsv\")\np &lt;- outgroup_f3 %&gt;%\n  ggplot() +\n  geom_point(aes(x = b, y = Estimate_Jackknife)) +\n  geom_errorbar(\n    aes(\n      x = b,\n      ymin = Estimate_Jackknife - StdErr_Jackknife,\n      ymax = Estimate_Jackknife + StdErr_Jackknife\n    ), width = 0.2\n  ) +\n  coord_flip() +\n  scale_x_discrete(limits = rev) +\n  theme(axis.title.y = element_blank())\n\n\n\n\nF-stats plot; estimated values and errors for each test population on the y-axis\n\n\n\n\n\n\n\n\nLearn more about…\n\n\n\n\nThe trident fetch -&gt; xerxes fstats workflow: Xerxes’s cutlery - enjoying your dinner?\nF-statistics and how to use xerxes: Introduction to F3- and F4-Statistics\nxerxes’ algorithms and implementation details: xerxes Whitepaper"
  },
  {
    "objectID": "fstats.html#footnotes",
    "href": "fstats.html#footnotes",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis test is inspired by Wang et al. (2023), Figure S1↩︎"
  },
  {
    "objectID": "ref_pca.html",
    "href": "ref_pca.html",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "With our data neatly packaged we can approach the actual challenge of reconstructing its genetic affiliation and origin story. To do so we require reference data to compare it to, for example with a principal component analysis.\n\n\nIn its public community-archive the Poseidon ecosystem features a curated set of publication-wise packages with ancient and modern genotype data.\nWe can get an overview over all packages available there with trident’s list subcommand if we employ the --remote flag for interaction with the open web API:\n\ntrident list --remote --packages\n\n\n\n\n\n\n\nNote\n\n\n\nA more interactive way to get an overview beyond the command line is the archive explorer on the Poseidon website. This small web app works also by querying the API.\n\n\nAs our sample of interest was found in Western Eurasia, a first valuable point of reference is genotype data from various modern populations from there. The following list includes a number of relevant populations. It can be downloaded here.\n\n\n\nmodern_western_eurasian_populations.txt\n\nAbazin\nAbkhasian\nAdygei\nAlbanian\nArmenian\nArmenian_Hemsheni\nAssyrian\nAvar\nAzeri\nBalkar\nBasque\nBedouinA\nBedouinB\nBelarusian\nBulgarian\nChechen\nCircassian\nCroatian\nCypriot\nCzech\nDarginian\nDruze\nEnglish\nEstonian\nEzid\nFinnish\nFrench\nGeorgian\nGreek\nHungarian\nIcelandic\nIngushian\nIranian\nItalian_North\nItalian_South\nJew_Ashkenazi\nJew_Georgian\nJew_Iranian\nJew_Iraqi\nJew_Libyan\nJew_Moroccan\nJew_Tunisian\nJew_Turkish\nJew_Yemenite\nJordanian\nKabardinian\nKaitag\nKarachai\nKubachinian\nKumyk\nKurd\nLak\nLebanese\nLebanese_Christian\nLebanese_Muslim\nLezgin\nLithuanian\nMaltese\nMoldavian\nMordovian\nNorwegian\nOrcadian\nOssetian\nPalestinian\nRomanian\nRussian\nSardinian\nSaudi\nScottish\nSicilian\nSpanish\nSpanish_North\nSyrian\nTabasaran\nTurkish\nUkrainian\n\n\n\nwget https://raw.githubusercontent.com/nevrome/mobest.analysis.2022/master/code/01_data_preparation/modern_western_eurasian_populations.txt\n\ntrident includes the fetch subcommand to download entire packages from the web API. Besides selecting entire packages for download, it also allows to identify and download packages that contain samples with a certain group attribution.\nWe can use this feature to download all packages that contain samples for the desired reference populations listed above.\n\ntrident fetch -d refData --fetchFile modern_western_eurasian_populations.txt\n\nrefData\n├── 2012_PattersonGenetics-2.1.3\n├── 2014_LazaridisNature-4.0.2\n├── 2016_LazaridisNature-2.1.3\n├── 2019_Biagini_Spain-2.2.1\n└── 2019_Jeong_InnerEurasia-3.0.1\nThis works, because the population list above already has the same structure as the selection language trident uses for fetch and forge.\n\n\n\nWe now have our own Poseidon package for the ice mummy in a directory ice and various packages with modern reference data in refData. Our package features genotype data in EIGENSTRAT format, the reference data packages include data in PLINKs binary format.\ntridents most notable feature is the ability to merge the exact samples we require into a new package from both data sources at once with the forge subcommand.\n\ntrident forge \\\n  -d refData \\\n  -d ice \\\n  --forgeFile modern_western_eurasian_populations.txt \\\n  -f \"&lt;ice&gt;\" \\\n  --outFormat EIGENSTRAT \\\n  --intersect \\\n  -o iceWithRef\n\nNote how we list two directories as data sources with -d/--baseDir. forge discovers and reads all Poseidon packages both in refData and ice.\nIt then constructs a subset- and merge operation, by parsing the selection language from both the --forgeFile modern_western_eurasian_populations.txt and the individual forge string -f \"&lt;ice&gt;\".\n\n\n\n\n\n\nNote\n\n\n\nThe forge language is a powerful DSL (domain specific language) that allows positive and negative selection of individuals (in &lt;...&gt;), populations and Poseidon packages (in *...*). It is documented on the Poseidon website here.\n\n\nforge reads both data in EIGENSTRAT and in PLINK’s binary format in constant memory via stream-processing. It can also produce both formats for the output package, though the default is the more space efficient PLINK format, so we have to set --outFormat EIGENSTRAT here for our downstream applications.\nThe --intersect flag is a reaction to the fact that our input packages feature different SNP sets: Some include only the ~600.000 HumanOrigins SNPs, others the ~1.240.000 1240K SNPs. As a default forge returns the union of all SNP sets it encounters, which would here yield many missing entries for the modern reference data.\n\n\n\n\n\n\nNote\n\n\n\ntrident forge can also create output packages with a specific SNP set using the --selectSnps option. This option takes either a .snp (EIGENSTRAT) or a .bim (PLINK) file with the desired SNPs. Any SNP not listed in the file will be excluded.\n\n\nWhen the forge process has completed we can inspect the result with the summarize subcommand:\n\ntrident summarize -d iceWithRef\n\n\n\n\nOur iceWithRef package is ready for downstream analysis, but it is also a bit bulky. It includes, for example, 314 Russian samples alone. We could probably reduce its size without loosing much statistical power for the PCA we want to run below.\nOne way of doing so would be to limit the maximum number of samples per population. We can easily subset the package with trident forge, if we can obtain a list of samples we want to keep. But how do we get this list?\nThe Poseidon framework includes the qjanno software tool that allows to query arbitrary .janno and .txt files as SQLite database tables and thus enables advanced selection operations.\nWe can install it just as trident:\n\n# download the current stable release binary\nwget https://github.com/poseidon-framework/qjanno/releases/latest/download/qjanno-Linux\n# rename it to simply qjanno\nmv qjanno-Linux qjanno\n# make it executable\nchmod +x qjanno\n# run it to test if it is working\n./qjanno -h\n\nWith qjanno ready we can query .janno files for arbitrary information, e.g. the number of samples from each country.\n\n./qjanno \"\nSELECT   Country,\n         count(*) as n\nFROM     d(iceWithRef)\nGROUP BY Country\nORDER BY n\n\"\n\n\n\n\n\n\n\nNote\n\n\n\nThe d() pseudo-function in the FROM ... field of the query is one of three pseudo-functions to automatically search for .janno files to load them into qjanno. Read more about that here.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we forget the available columns in .janno files the -c option returns a list for us:\n\nqjanno \"SELECT * FROM d(iceWithRef)\" -c\n\n\n\nWith this power we can construct a two-step query for random samples for each group.\n\n# construct a randomly ordered list of groups and their samples\nqjanno \"\nSELECT Poseidon_ID,\n       Group_Name,\n       row_number() OVER (PARTITION BY Group_Name ORDER BY random()) rn\nFROM   d(iceWithRef)\n\" --raw &gt; id_per_sample.txt\ncat id_per_sample.txt\n# subset samples from that list and render a forge selection language file\nqjanno \"\nSELECT   '&lt;'||Poseidon_ID||'&gt;'\nFROM     id_per_sample.txt\nWHERE    rn &lt;= 10\nORDER BY Group_Name DESC;\n\" --raw --noOutHeader &gt; ten_samples_max.txt\n\nten_samples_max.txt again has the structure of the forge selection language and we can run it with it to derive a smaller analysis dataset.\n\ntrident forge \\\n  -d iceWithRef \\\n  --forgeFile ten_samples_max.txt \\\n  --outFormat EIGENSTRAT \\\n  -o iceWithRefSmall\n\ntrident summarize -d iceWithRefSmall\n\n\n\n\n\n\n\nNote\n\n\n\nNote that iceWithRefSmall includes a .bib file with exactly these references needed for the (random) subset of samples we selected now. Literature references are linked to samples in Poseidon packages and forge moves them accordingly to newly created packages.\nWhen used consistently throughout the analysis pipeline this feature can simplify the bibliography compilation at the end.\n\n\n\n\n\nAfter all these technicalities we can finally go back to the scientific question that motivated all of this: What is the genetic profile of the ice mummy Prof. P. had found in the Tyrolean Alps?\nHere we use the smart_pca() function from the smartsnp R package1.\n\n# read the ind file\nind &lt;- readr::read_tsv(\n  \"scratch/iceWithRefSmall/iceWithRefSmall.ind\",\n  col_names = c(\"id\", \"sex\", \"pop\")\n)\n# run smart_snp() with the iceWithRefSmall dataset\npca_out &lt;- smartsnp::smart_pca(\n  \"scratch/iceWithRefSmall/iceWithRefSmall.geno\",\n  sample_group = seq_len(nrow(ind)),\n  missing_impute = \"mean\",\n  # project the \"ice\" sample in the pca space constructed\n  # with modern reference data\n  sample_project = which(ind$id == \"ice\"),\n  pc_axes = 2\n)\n\nWe should probably save this intermediate result in the file system.\n\nsaveRDS(pca_out, file = \"scratch/pca_out.rds\")\n#pca_out &lt;- readRDS(\"scratch/pca_out.rds\")\n\nAnd we can finally plot it like this:\n\nlibrary(magrittr)\nlibrary(ggplot2)\n\np &lt;- pca_out$pca.sample_coordinates %&gt;%\n  ggplot() +\n  geom_point(\n    aes(x = PC1, y = PC2, color = Class)\n  ) +\n  scale_y_reverse() +\n  coord_fixed()\n\n\n\n\nPCA plot; the modern reference samples are marked in red and the projected ice mummy sample in blue\n\n\n\n\n\n\n\n\n\nLearn more about…\n\n\n\n\nThe Poseidon public archives: Public Poseidon archives\nThe forge selection language: Trident’s restaurant - your order please?\nThe qjanno software tool: qjanno CLI software\nPCA with the smartsnp R package: Vignette: Projecting ancient samples"
  },
  {
    "objectID": "ref_pca.html#analyzing-our-data-with-reference-data",
    "href": "ref_pca.html#analyzing-our-data-with-reference-data",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "",
    "text": "With our data neatly packaged we can approach the actual challenge of reconstructing its genetic affiliation and origin story. To do so we require reference data to compare it to, for example with a principal component analysis.\n\n\nIn its public community-archive the Poseidon ecosystem features a curated set of publication-wise packages with ancient and modern genotype data.\nWe can get an overview over all packages available there with trident’s list subcommand if we employ the --remote flag for interaction with the open web API:\n\ntrident list --remote --packages\n\n\n\n\n\n\n\nNote\n\n\n\nA more interactive way to get an overview beyond the command line is the archive explorer on the Poseidon website. This small web app works also by querying the API.\n\n\nAs our sample of interest was found in Western Eurasia, a first valuable point of reference is genotype data from various modern populations from there. The following list includes a number of relevant populations. It can be downloaded here.\n\n\n\nmodern_western_eurasian_populations.txt\n\nAbazin\nAbkhasian\nAdygei\nAlbanian\nArmenian\nArmenian_Hemsheni\nAssyrian\nAvar\nAzeri\nBalkar\nBasque\nBedouinA\nBedouinB\nBelarusian\nBulgarian\nChechen\nCircassian\nCroatian\nCypriot\nCzech\nDarginian\nDruze\nEnglish\nEstonian\nEzid\nFinnish\nFrench\nGeorgian\nGreek\nHungarian\nIcelandic\nIngushian\nIranian\nItalian_North\nItalian_South\nJew_Ashkenazi\nJew_Georgian\nJew_Iranian\nJew_Iraqi\nJew_Libyan\nJew_Moroccan\nJew_Tunisian\nJew_Turkish\nJew_Yemenite\nJordanian\nKabardinian\nKaitag\nKarachai\nKubachinian\nKumyk\nKurd\nLak\nLebanese\nLebanese_Christian\nLebanese_Muslim\nLezgin\nLithuanian\nMaltese\nMoldavian\nMordovian\nNorwegian\nOrcadian\nOssetian\nPalestinian\nRomanian\nRussian\nSardinian\nSaudi\nScottish\nSicilian\nSpanish\nSpanish_North\nSyrian\nTabasaran\nTurkish\nUkrainian\n\n\n\nwget https://raw.githubusercontent.com/nevrome/mobest.analysis.2022/master/code/01_data_preparation/modern_western_eurasian_populations.txt\n\ntrident includes the fetch subcommand to download entire packages from the web API. Besides selecting entire packages for download, it also allows to identify and download packages that contain samples with a certain group attribution.\nWe can use this feature to download all packages that contain samples for the desired reference populations listed above.\n\ntrident fetch -d refData --fetchFile modern_western_eurasian_populations.txt\n\nrefData\n├── 2012_PattersonGenetics-2.1.3\n├── 2014_LazaridisNature-4.0.2\n├── 2016_LazaridisNature-2.1.3\n├── 2019_Biagini_Spain-2.2.1\n└── 2019_Jeong_InnerEurasia-3.0.1\nThis works, because the population list above already has the same structure as the selection language trident uses for fetch and forge.\n\n\n\nWe now have our own Poseidon package for the ice mummy in a directory ice and various packages with modern reference data in refData. Our package features genotype data in EIGENSTRAT format, the reference data packages include data in PLINKs binary format.\ntridents most notable feature is the ability to merge the exact samples we require into a new package from both data sources at once with the forge subcommand.\n\ntrident forge \\\n  -d refData \\\n  -d ice \\\n  --forgeFile modern_western_eurasian_populations.txt \\\n  -f \"&lt;ice&gt;\" \\\n  --outFormat EIGENSTRAT \\\n  --intersect \\\n  -o iceWithRef\n\nNote how we list two directories as data sources with -d/--baseDir. forge discovers and reads all Poseidon packages both in refData and ice.\nIt then constructs a subset- and merge operation, by parsing the selection language from both the --forgeFile modern_western_eurasian_populations.txt and the individual forge string -f \"&lt;ice&gt;\".\n\n\n\n\n\n\nNote\n\n\n\nThe forge language is a powerful DSL (domain specific language) that allows positive and negative selection of individuals (in &lt;...&gt;), populations and Poseidon packages (in *...*). It is documented on the Poseidon website here.\n\n\nforge reads both data in EIGENSTRAT and in PLINK’s binary format in constant memory via stream-processing. It can also produce both formats for the output package, though the default is the more space efficient PLINK format, so we have to set --outFormat EIGENSTRAT here for our downstream applications.\nThe --intersect flag is a reaction to the fact that our input packages feature different SNP sets: Some include only the ~600.000 HumanOrigins SNPs, others the ~1.240.000 1240K SNPs. As a default forge returns the union of all SNP sets it encounters, which would here yield many missing entries for the modern reference data.\n\n\n\n\n\n\nNote\n\n\n\ntrident forge can also create output packages with a specific SNP set using the --selectSnps option. This option takes either a .snp (EIGENSTRAT) or a .bim (PLINK) file with the desired SNPs. Any SNP not listed in the file will be excluded.\n\n\nWhen the forge process has completed we can inspect the result with the summarize subcommand:\n\ntrident summarize -d iceWithRef\n\n\n\n\nOur iceWithRef package is ready for downstream analysis, but it is also a bit bulky. It includes, for example, 314 Russian samples alone. We could probably reduce its size without loosing much statistical power for the PCA we want to run below.\nOne way of doing so would be to limit the maximum number of samples per population. We can easily subset the package with trident forge, if we can obtain a list of samples we want to keep. But how do we get this list?\nThe Poseidon framework includes the qjanno software tool that allows to query arbitrary .janno and .txt files as SQLite database tables and thus enables advanced selection operations.\nWe can install it just as trident:\n\n# download the current stable release binary\nwget https://github.com/poseidon-framework/qjanno/releases/latest/download/qjanno-Linux\n# rename it to simply qjanno\nmv qjanno-Linux qjanno\n# make it executable\nchmod +x qjanno\n# run it to test if it is working\n./qjanno -h\n\nWith qjanno ready we can query .janno files for arbitrary information, e.g. the number of samples from each country.\n\n./qjanno \"\nSELECT   Country,\n         count(*) as n\nFROM     d(iceWithRef)\nGROUP BY Country\nORDER BY n\n\"\n\n\n\n\n\n\n\nNote\n\n\n\nThe d() pseudo-function in the FROM ... field of the query is one of three pseudo-functions to automatically search for .janno files to load them into qjanno. Read more about that here.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we forget the available columns in .janno files the -c option returns a list for us:\n\nqjanno \"SELECT * FROM d(iceWithRef)\" -c\n\n\n\nWith this power we can construct a two-step query for random samples for each group.\n\n# construct a randomly ordered list of groups and their samples\nqjanno \"\nSELECT Poseidon_ID,\n       Group_Name,\n       row_number() OVER (PARTITION BY Group_Name ORDER BY random()) rn\nFROM   d(iceWithRef)\n\" --raw &gt; id_per_sample.txt\ncat id_per_sample.txt\n# subset samples from that list and render a forge selection language file\nqjanno \"\nSELECT   '&lt;'||Poseidon_ID||'&gt;'\nFROM     id_per_sample.txt\nWHERE    rn &lt;= 10\nORDER BY Group_Name DESC;\n\" --raw --noOutHeader &gt; ten_samples_max.txt\n\nten_samples_max.txt again has the structure of the forge selection language and we can run it with it to derive a smaller analysis dataset.\n\ntrident forge \\\n  -d iceWithRef \\\n  --forgeFile ten_samples_max.txt \\\n  --outFormat EIGENSTRAT \\\n  -o iceWithRefSmall\n\ntrident summarize -d iceWithRefSmall\n\n\n\n\n\n\n\nNote\n\n\n\nNote that iceWithRefSmall includes a .bib file with exactly these references needed for the (random) subset of samples we selected now. Literature references are linked to samples in Poseidon packages and forge moves them accordingly to newly created packages.\nWhen used consistently throughout the analysis pipeline this feature can simplify the bibliography compilation at the end.\n\n\n\n\n\nAfter all these technicalities we can finally go back to the scientific question that motivated all of this: What is the genetic profile of the ice mummy Prof. P. had found in the Tyrolean Alps?\nHere we use the smart_pca() function from the smartsnp R package1.\n\n# read the ind file\nind &lt;- readr::read_tsv(\n  \"scratch/iceWithRefSmall/iceWithRefSmall.ind\",\n  col_names = c(\"id\", \"sex\", \"pop\")\n)\n# run smart_snp() with the iceWithRefSmall dataset\npca_out &lt;- smartsnp::smart_pca(\n  \"scratch/iceWithRefSmall/iceWithRefSmall.geno\",\n  sample_group = seq_len(nrow(ind)),\n  missing_impute = \"mean\",\n  # project the \"ice\" sample in the pca space constructed\n  # with modern reference data\n  sample_project = which(ind$id == \"ice\"),\n  pc_axes = 2\n)\n\nWe should probably save this intermediate result in the file system.\n\nsaveRDS(pca_out, file = \"scratch/pca_out.rds\")\n#pca_out &lt;- readRDS(\"scratch/pca_out.rds\")\n\nAnd we can finally plot it like this:\n\nlibrary(magrittr)\nlibrary(ggplot2)\n\np &lt;- pca_out$pca.sample_coordinates %&gt;%\n  ggplot() +\n  geom_point(\n    aes(x = PC1, y = PC2, color = Class)\n  ) +\n  scale_y_reverse() +\n  coord_fixed()\n\n\n\n\nPCA plot; the modern reference samples are marked in red and the projected ice mummy sample in blue\n\n\n\n\n\n\n\n\n\nLearn more about…\n\n\n\n\nThe Poseidon public archives: Public Poseidon archives\nThe forge selection language: Trident’s restaurant - your order please?\nThe qjanno software tool: qjanno CLI software\nPCA with the smartsnp R package: Vignette: Projecting ancient samples"
  },
  {
    "objectID": "ref_pca.html#footnotes",
    "href": "ref_pca.html#footnotes",
    "title": "Poseidon introduction for Tübingen University, February 2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHerrando‐Pérez, Tobler, and Huber (2021)↩︎"
  }
]