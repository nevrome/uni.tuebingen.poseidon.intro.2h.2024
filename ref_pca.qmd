---
editor_options: 
  chunk_output_type: inline
execute: 
  eval: false
engine: knitr
shift-heading-level-by: 1
---

# Analyzing our data with reference data

With our data neatly packaged we can approach the actual challenge of reconstructing its genetic affiliation and origin story. To do so we require reference data to compare it to, for example with a principal component analysis.

## Acquiring modern reference data

In its public [community-archive](https://www.poseidon-adna.org/#/archive_overview) the Poseidon ecosystem features a curated set of publication-wise packages with ancient and modern genotype data.

We can get an overview over all packages available there with `trident`'s [`list`](https://www.poseidon-adna.org/#/trident?id=list-command) subcommand if we employ the `--remote` flag for interaction with the open web API:

```{bash}
trident list --remote --packages
```

::: {.callout-note}
A more interactive way to get an overview beyond the command line is the [archive explorer](https://www.poseidon-adna.org/#/archive_explorer) on the Poseidon website. This small web app works also by querying the API.
:::

As our sample of interest was found in Western Eurasia, a first valuable point of reference is genotype data from various modern populations from there. The following list includes a number of relevant populations. It can be downloaded [here](https://raw.githubusercontent.com/nevrome/mobest.analysis.2022/master/code/01_data_preparation/modern_western_eurasian_populations.txt).

:::{.scrolling}

```{filename="modern_western_eurasian_populations.txt"}
Abazin
Abkhasian
Adygei
Albanian
Armenian
Armenian_Hemsheni
Assyrian
Avar
Azeri
Balkar
Basque
BedouinA
BedouinB
Belarusian
Bulgarian
Chechen
Circassian
Croatian
Cypriot
Czech
Darginian
Druze
English
Estonian
Ezid
Finnish
French
Georgian
Greek
Hungarian
Icelandic
Ingushian
Iranian
Italian_North
Italian_South
Jew_Ashkenazi
Jew_Georgian
Jew_Iranian
Jew_Iraqi
Jew_Libyan
Jew_Moroccan
Jew_Tunisian
Jew_Turkish
Jew_Yemenite
Jordanian
Kabardinian
Kaitag
Karachai
Kubachinian
Kumyk
Kurd
Lak
Lebanese
Lebanese_Christian
Lebanese_Muslim
Lezgin
Lithuanian
Maltese
Moldavian
Mordovian
Norwegian
Orcadian
Ossetian
Palestinian
Romanian
Russian
Sardinian
Saudi
Scottish
Sicilian
Spanish
Spanish_North
Syrian
Tabasaran
Turkish
Ukrainian
```

:::

```{bash}
wget https://raw.githubusercontent.com/nevrome/mobest.analysis.2022/master/code/01_data_preparation/modern_western_eurasian_populations.txt
```

`trident` includes the [`fetch`](https://www.poseidon-adna.org/#/trident?id=fetch-command) subcommand to download entire packages from the web API. Besides selecting entire packages for download, it also allows to identify and download packages that contain samples with a certain group attribution.

We can use this feature to download all packages that contain samples for the desired reference populations listed above.

```{bash}
trident fetch -d refData --fetchFile modern_western_eurasian_populations.txt
```

```
refData
├── 2012_PattersonGenetics-2.1.3
├── 2014_LazaridisNature-4.0.2
├── 2016_LazaridisNature-2.1.3
├── 2019_Biagini_Spain-2.2.1
└── 2019_Jeong_InnerEurasia-3.0.1
```

This works, because the population list above already has the same structure as the selection language `trident` uses for `fetch` and `forge`.

## Merging the reference data with our data

We now have our own Poseidon package for the ice mummy in a directory `ice` and various packages with modern reference data in `refData`. Our package features genotype data in EIGENSTRAT format, the reference data packages include data in PLINKs binary format.

`trident`s most notable feature is the ability to merge the exact samples we require into a new package from both data sources at once with the [`forge`](https://www.poseidon-adna.org/#/trident?id=forge-command) subcommand.

```{bash}
trident forge \
  -d refData \
  -d ice \
  --forgeFile modern_western_eurasian_populations.txt \
  -f "<ice>" \
  --outFormat EIGENSTRAT \
  --intersect \
  -o iceWithRef
```

Note how we list two directories as data sources with `-d`/`--baseDir`. `forge` discovers and reads all Poseidon packages both in `refData` and `ice`.

It then constructs a subset- and merge operation, by parsing the selection language from both the `--forgeFile` `modern_western_eurasian_populations.txt` and the individual forge string `-f` `"<ice>"`. 

::: {.callout-note}
The `forge` language is a powerful DSL (domain specific language) that allows positive and negative selection of individuals (in `<...>`), populations and Poseidon packages (in `*...*`). It is documented on the Poseidon website [here](https://www.poseidon-adna.org/#/trident?id=the-forge-selection-language).
:::

`forge` reads both data in EIGENSTRAT and in PLINK's binary format in constant memory via stream-processing. It can also produce both formats for the output package, though the default is the more space efficient PLINK format, so we have to set `--outFormat EIGENSTRAT` here for our downstream applications.

The `--intersect` flag is a reaction to the fact that our input packages feature different SNP sets: Some include only the ~600.000 HumanOrigins SNPs, others the ~1.240.000 1240K SNPs. As a default `forge` returns the union of all SNP sets it encounters, which would here yield many missing entries for the modern reference data.

::: {.callout-note}
`trident forge` can also create output packages with a specific SNP set using the `--selectSnps` option. This option takes either a `.snp` (EIGENSTRAT) or a `.bim` (PLINK) file with the desired SNPs. Any SNP not listed in the file will be excluded.
:::

When the forge process has completed we can inspect the result with the [`summarize`](https://www.poseidon-adna.org/#/trident?id=summarise-command) subcommand:

```{bash}
trident summarize -d iceWithRef
```

## Reducing the data size

Our `iceWithRef` package is ready for downstream analysis, but it is also a bit bulky. It includes, for example, 314 Russian samples alone. We could probably reduce its size without loosing much statistical power for the PCA we want to run below.

One way of doing so would be to limit the maximum number of samples per population. We can easily subset the package with `trident forge`, if we can obtain a list of samples we want to keep. But how do we get this list?

The Poseidon framework includes the [`qjanno`](https://www.poseidon-adna.org/#/qjanno) software tool that allows to query arbitrary `.janno` and `.txt` files as SQLite database tables and thus enables advanced selection operations.

We can install it just as `trident`:

```{bash}
# download the current stable release binary
wget https://github.com/poseidon-framework/qjanno/releases/latest/download/qjanno-Linux
# rename it to simply qjanno
mv qjanno-Linux qjanno
# make it executable
chmod +x qjanno
# run it to test if it is working
./qjanno -h
```

With `qjanno` ready we can query `.janno` files for arbitrary information, e.g. the number of samples from each country.

```{bash}
./qjanno "
SELECT   Country,
         count(*) as n
FROM     d(iceWithRef)
GROUP BY Country
ORDER BY n
"
```

::: {.callout-note}
The `d()` pseudo-function in the `FROM ...` field of the query is one of three pseudo-functions to automatically search for `.janno` files to load them into `qjanno`. Read more about that [here](https://www.poseidon-adna.org/#/qjanno?id=the-janno-crawling-pseudo-functions).
:::

::: {.callout-note}
If we forget the available columns in `.janno` files the `-c` option returns a list for us:

```{bash}
qjanno "SELECT * FROM d(iceWithRef)" -c
```
:::

With this power we can construct a two-step query for random samples for each group. 

```{bash}
# construct a randomly ordered list of groups and their samples
qjanno "
SELECT Poseidon_ID,
       Group_Name,
       row_number() OVER (PARTITION BY Group_Name ORDER BY random()) rn
FROM   d(iceWithRef)
" --raw > id_per_sample.txt
cat id_per_sample.txt
# subset samples from that list and render a forge selection language file
qjanno "
SELECT   '<'||Poseidon_ID||'>'
FROM     id_per_sample.txt
WHERE    rn <= 10
ORDER BY Group_Name DESC;
" --raw --noOutHeader > ten_samples_max.txt
```

`ten_samples_max.txt` again has the structure of the `forge` selection language and we can run it with it to derive a smaller analysis dataset.

```{bash}
trident forge \
  -d iceWithRef \
  --forgeFile ten_samples_max.txt \
  --outFormat EIGENSTRAT \
  -o iceWithRefSmall

trident summarize -d iceWithRefSmall
```

::: {.callout-note}
Note that `iceWithRefSmall` includes a `.bib` file with exactly these references needed for the (random) subset of samples we selected now. Literature references are linked to samples in Poseidon packages and `forge` moves them accordingly to newly created packages.

When used consistently throughout the analysis pipeline this feature can simplify the bibliography compilation at the end.
:::

## Performing a PCA analysis

After all these technicalities we can finally go back to the scientific question that motivated all of this: What is the genetic profile of the ice mummy Prof. P. had found in the Tyrolean Alps?

Here we use the `smart_pca()` function from the [`smartsnp`](https://github.com/ChristianHuber/smartsnp) R package^[@HerrandoPrez2021].

```{r}
# read the ind file
ind <- readr::read_tsv(
  "scratch/iceWithRefSmall/iceWithRefSmall.ind",
  col_names = c("id", "sex", "pop")
)
# run smart_snp() with the iceWithRefSmall dataset
pca_out <- smartsnp::smart_pca(
  "scratch/iceWithRefSmall/iceWithRefSmall.geno",
  sample_group = seq_len(nrow(ind)),
  missing_impute = "mean",
  # project the "ice" sample in the pca space constructed
  # with modern reference data
  sample_project = which(ind$id == "ice"),
  pc_axes = 2
)
```

We should probably save this intermediate result in the file system.

```{r}
saveRDS(pca_out, file = "scratch/pca_out.rds")
#pca_out <- readRDS("scratch/pca_out.rds")
```

And we can finally plot it like this:

```{r}
library(magrittr)
library(ggplot2)

p <- pca_out$pca.sample_coordinates %>%
  ggplot() +
  geom_point(
    aes(x = PC1, y = PC2, color = Class)
  ) +
  scale_y_reverse() +
  coord_fixed()
```

```{r, echo=FALSE}
ggsave(
  "img/pca1.png",
  plot = p,
  width = 600, height = 400,
  dpi = 150,
  units = "px",
  scale = 1.2
)
```

![PCA plot; the modern reference samples are marked in red and the projected ice mummy sample in blue](img/pca1.png)

***

::: {.callout-tip}
## Learn more about...
- The Poseidon public archives: [Public Poseidon archives](https://www.poseidon-adna.org/#/archive_overview)
- The `forge` selection language: [Trident’s restaurant - your order please?](https://blog.poseidon-adna.org/posts/trident_14.html)
- The `qjanno` software tool: [qjanno CLI software](https://www.poseidon-adna.org/#/qjanno)
- PCA with the `smartsnp` R package: [Vignette: Projecting ancient samples](https://christianhuber.github.io/smartsnp/articles/aDNA_smartpca_analysis.html)
:::
