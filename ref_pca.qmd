---
editor_options: 
  chunk_output_type: inline
execute: 
  eval: false
engine: knitr
shift-heading-level-by: 1
---

# Analyzing our data with reference data

With our data neatly packaged we can approach the actual challenge of reconstructing its genetic affiliation and origin story. To do so we require reference data to compare it to, e.g. with a principal component analysis.

## Acquiring modern reference data

In its public [community-archive](https://www.poseidon-adna.org/#/archive_overview) the Poseidon ecosystem features a curated source of genotype data.

We can get an overview over all publication-wise packages available there with `trident`:

```{bash}
./trident list --remote --packages
```

Or in a more interactive way with the [archive explorer](https://www.poseidon-adna.org/#/archive_explorer) on the poseidon website.

As our sample of interest was found in Western Eurasia a first valuable point of reference is genotype data from various modern populations from there.

:::{.scrolling}

```{filename="modern_western_eurasian_populations.txt"}
Abazin
Abkhasian
Adygei
Albanian
Armenian
Armenian_Hemsheni
Assyrian
Avar
Azeri
Balkar
Basque
BedouinA
BedouinB
Belarusian
Bulgarian
Chechen
Circassian
Croatian
Cypriot
Czech
Darginian
Druze
English
Estonian
Ezid
Finnish
French
Georgian
Greek
Hungarian
Icelandic
Ingushian
Iranian
Italian_North
Italian_South
Jew_Ashkenazi
Jew_Georgian
Jew_Iranian
Jew_Iraqi
Jew_Libyan
Jew_Moroccan
Jew_Tunisian
Jew_Turkish
Jew_Yemenite
Jordanian
Kabardinian
Kaitag
Karachai
Kubachinian
Kumyk
Kurd
Lak
Lebanese
Lebanese_Christian
Lebanese_Muslim
Lezgin
Lithuanian
Maltese
Moldavian
Mordovian
Norwegian
Orcadian
Ossetian
Palestinian
Romanian
Russian
Sardinian
Saudi
Scottish
Sicilian
Spanish
Spanish_North
Syrian
Tabasaran
Turkish
Ukrainian
```

The list can be downloaded [here](https://raw.githubusercontent.com/nevrome/mobest.analysis.2022/master/code/01_data_preparation/modern_western_eurasian_populations.txt).

:::

`trident` allows to download packages that contain samples with this group attribution from the community archive.

```{bash}
trident fetch -d refData --fetchFile modern_western_eurasian_populations.txt
```

```
refData
├── 2012_PattersonGenetics-2.1.3
├── 2014_LazaridisNature-4.0.2
├── 2016_LazaridisNature-2.1.3
├── 2019_Biagini_Spain-2.2.1
└── 2019_Jeong_InnerEurasia-3.0.1
```

This works, because the population list above already has the same structure as the selection language trident uses for `fetch` and `forge` (more about that below).

## Merging the reference data with our data

We now have our own Poseidon package for the ice mummy in a directory `ice` and various packages with modern reference data in `refData`. Our package features genotype data in EIGENSTRAT format, the reference data packages include data in PLINKs binary format.

`trident`s most notable feature is the ability to merge the exact samples we require from both data sources at once with the [`forge` subcommand](https://www.poseidon-adna.org/#/trident?id=forge-command).

```{bash}
trident forge \
  -d refData \
  -d ice \
  --forgeFile modern_western_eurasian_populations.txt \
  -f "<ice>" \
  --outFormat EIGENSTRAT \
  --intersect \
  -o iceWithRef
```

Note how we list two directories as data sources with `-d`/`--baseDir`. `forge` discovers and reads all Poseidon packages both in `refData` and `ice`.

It then constructs a subset- and merge operation, by parsing the selection language from both the `--forgeFile` `modern_western_eurasian_populations.txt` and the individual forge string `-f` `"<ice>"`. The forge language is a powerful DSL (domain specific language) that allows positive and negative selection of individuals (in `<...>`), populations and Poseidon packages (in `*...*`).

`forge` reads both data in EIGENSTRAT and in PLINK's binary format in constant memory via stream-processing. It can also produce both formats for the output package, though the default is the more space efficient PLINK format, so we have to set `--outFormat EIGENSTRAT` here for our downstream applications.

The `--intersect` flag is a reaction to the fact that our input packages feature different SNP sets: Some include only the ~600.000k HumanOrigins SNPs, others the ~1.240.000k 1240K SNPs. As a default `forge` returns the union of all SNP sets it encounters, which would here yield many missing entries for the modern reference data.

```{bash}
trident summarize -d iceWithRef
```

## Reducing the data size

Our `iceWithRef` package is ready for downstream analysis, but it is also a bit bulky with, for example, 314 Russian samples alone. We could probably reduce its size without loosing much statistical power for the PCA we want to run.

One way of doing so would be, to limit the maximum number of samples per population. We can easily subset the package with `trident forge`, if we can obtain a list of samples we want to keep. But how do we get this list?

The Poseidon framework includes the [`qjanno` software tool](https://www.poseidon-adna.org/#/qjanno) that allows to query arbitrary .janno and .txt files as SQLite database tables and thus enable advanced selection operations.

We can install it just as `trident`:

```{bash}
# download the current stable release binary
wget https://github.com/poseidon-framework/qjanno/releases/latest/download/qjanno-Linux
# rename it to simply qjanno
mv qjanno-Linux qjanno
# make it executable
chmod +x qjanno
# run it to test if it is working
./qjanno -h
```

With qjanno ready we can query the .janno file for arbitrary information, e.g. the number of samples from each country.

```{bash}
./qjanno "
SELECT   Country,
         count(*) as n
FROM     d(iceWithRef)
GROUP BY Country
ORDER BY n
"
```

If we forget the available columns in .janno files the `-c` option returns a list.

```{bash}
qjanno "SELECT * FROM d(iceWithRef)" -c
```

With this power we can construct a two-step query for random samples for each group. 

```{bash}
# construct a randomly ordered list of groups and their samples
qjanno "
SELECT Poseidon_ID,
       Group_Name,
       row_number() OVER (PARTITION BY Group_Name ORDER BY random()) rn
FROM   d(iceWithRef)
" --raw > id_per_sample.txt
cat id_per_sample.txt
# subset samples from that list
qjanno "
SELECT   '<'||Poseidon_ID||'>'
FROM     id_per_sample.txt
WHERE    rn <= 10
ORDER BY Group_Name DESC;
" --raw --noOutHeader > ten_samples_max.txt
```

`ten_samples_max.txt` has the structure of the selection language for `forge` and we can run it with it to derive a smaller analysis dataset.

```{bash}
trident forge \
  -d iceWithRef \
  --forgeFile ten_samples_max.txt \
  --outFormat EIGENSTRAT \
  -o iceWithRefSmall
```

```{bash}
trident summarize -d iceWithRefSmall
```

## Perform a PCA analysis

```{r}
ind <- readr::read_tsv(
  "scratch/myDataWithRef/myDataWithRef.ind",
  col_names = c("id", "sex", "pop")
)

pca_out <- smartsnp::smart_pca(
  "scratch/myDataWithRef/myDataWithRef.geno",
  sample_group = seq_len(nrow(ind)),
  missing_impute = "mean",
  sample_project = which(ind$id == "Iceman.SG"),
  pc_axes = 2
)

saveRDS(pca_out, file = "scratch/pca_out.rds")
```

```{r}
pca_out <- readRDS("scratch/pca_out.rds")

library(magrittr)
library(ggplot2)

pca_out$pca.sample_coordinates %>%
  ggplot() +
  geom_point(aes(x = PC1, y = PC2, color = Class))
```
