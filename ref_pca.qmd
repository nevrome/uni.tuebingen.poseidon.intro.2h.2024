---
editor_options: 
  chunk_output_type: inline
execute: 
  eval: false
engine: knitr
shift-heading-level-by: 1
---

# Analyzing our data with reference data

With our data neatly packaged we can approach the actual challenge of reconstructing its genetic affiliation and origin story. To do so we require reference data to compare it to, e.g. with a principal component analysis.

## Acquiring modern reference data

In its public [community-archive](https://www.poseidon-adna.org/#/archive_overview) the Poseidon ecosystem features a curated source of genotype data.

We can get an overview over all publication-wise packages available there with `trident`:

```{bash}
./trident list --remote --packages
```

Or in a more interactive way with the [archive explorer](https://www.poseidon-adna.org/#/archive_explorer) on the poseidon website.

As our sample of interest was found in Western Eurasia a first valuable point of reference is genotype data from various modern populations from there.

:::{.scrolling}

```{filename="modern_western_eurasian_populations.txt"}
Abazin
Abkhasian
Adygei
Albanian
Armenian
Armenian_Hemsheni
Assyrian
Avar
Azeri
Balkar
Basque
BedouinA
BedouinB
Belarusian
Bulgarian
Chechen
Circassian
Croatian
Cypriot
Czech
Darginian
Druze
English
Estonian
Ezid
Finnish
French
Georgian
Greek
Hungarian
Icelandic
Ingushian
Iranian
Italian_North
Italian_South
Jew_Ashkenazi
Jew_Georgian
Jew_Iranian
Jew_Iraqi
Jew_Libyan
Jew_Moroccan
Jew_Tunisian
Jew_Turkish
Jew_Yemenite
Jordanian
Kabardinian
Kaitag
Karachai
Kubachinian
Kumyk
Kurd
Lak
Lebanese
Lebanese_Christian
Lebanese_Muslim
Lezgin
Lithuanian
Maltese
Moldavian
Mordovian
Norwegian
Orcadian
Ossetian
Palestinian
Romanian
Russian
Sardinian
Saudi
Scottish
Sicilian
Spanish
Spanish_North
Syrian
Tabasaran
Turkish
Ukrainian
```

The list can be downloaded [here](https://raw.githubusercontent.com/nevrome/mobest.analysis.2022/master/code/01_data_preparation/modern_western_eurasian_populations.txt).

:::

`trident` allows to download packages that contain samples with this group attribution from the community archive.

```{bash}
trident fetch -d refData --fetchFile modern_western_eurasian_populations.txt
```

This works, because the population list above already has the same structure as the selection language trident uses for `fetch` and `forge` (more about that below).

## Merging the reference data with our data

```{bash}
trident forge \
  -d data \
  --forgeFile modern_western_eurasian_populations.txt \
  -p myOwnData/myOwnData.bed \
  -f "<Iceman.SG>,<RISE434.SG>" \
  --intersect \
  --outFormat EIGENSTRAT \
  -o myDataWithRef
```

```{bash}
qjanno "SELECT * FROM d(myDataWithRef)" -c

qjanno "
SELECT Poseidon_ID, Group_Name, row_number() OVER (PARTITION BY Group_Name ORDER BY random()) rn
FROM d(myDataWithRef)
" --raw > huhu.txt

qjanno "
SELECT '<'||Poseidon_ID||'>'
FROM huhu.txt
WHERE rn <= 2
ORDER BY Group_Name DESC;
" --raw --noOutHeader | wc -l
```

```{bash}
trident summarise -d myPac
```

```{bash}
trident survey -d myPac
```

```{bash}
#trident genoconvert -d myPac --outFormat EIGENSTRAT
```

.bib file

```{r}
ind <- readr::read_tsv(
  "scratch/myDataWithRef/myDataWithRef.ind",
  col_names = c("id", "sex", "pop")
)

pca_out <- smartsnp::smart_pca(
  "scratch/myDataWithRef/myDataWithRef.geno",
  sample_group = seq_len(nrow(ind)),
  missing_impute = "mean",
  sample_project = which(ind$id == "Iceman.SG"),
  pc_axes = 2
)

saveRDS(pca_out, file = "scratch/pca_out.rds")
```

```{r}
pca_out <- readRDS("scratch/pca_out.rds")

library(magrittr)
library(ggplot2)

pca_out$pca.sample_coordinates %>%
  ggplot() +
  geom_point(aes(x = PC1, y = PC2, color = Class))
```
