---
editor_options: 
  chunk_output_type: inline
execute: 
  eval: false
engine: knitr
shift-heading-level-by: 1
---

# Analyzing our data with reference data

With our data neatly packaged we can approach the actual challenge of reconstructing its genetic affiliation and origin story. To do so we require reference data to compare it to, e.g. with a principal component analysis.

## Acquiring modern reference data

In its public [community-archive](https://www.poseidon-adna.org/#/archive_overview) the Poseidon ecosystem features a curated source of genotype data.

We can get an overview over all publication-wise packages available there with `trident`:

```{bash}
./trident list --remote --packages
```

Or in a more interactive way with the [archive explorer](https://www.poseidon-adna.org/#/archive_explorer) on the poseidon website.

As our sample of interest was found in Western Eurasia a first valuable point of reference is genotype data from various modern populations from there.

:::{.scrolling}

```{filename="modern_western_eurasian_populations.txt"}
Abazin
Abkhasian
Adygei
Albanian
Armenian
Armenian_Hemsheni
Assyrian
Avar
Azeri
Balkar
Basque
BedouinA
BedouinB
Belarusian
Bulgarian
Chechen
Circassian
Croatian
Cypriot
Czech
Darginian
Druze
English
Estonian
Ezid
Finnish
French
Georgian
Greek
Hungarian
Icelandic
Ingushian
Iranian
Italian_North
Italian_South
Jew_Ashkenazi
Jew_Georgian
Jew_Iranian
Jew_Iraqi
Jew_Libyan
Jew_Moroccan
Jew_Tunisian
Jew_Turkish
Jew_Yemenite
Jordanian
Kabardinian
Kaitag
Karachai
Kubachinian
Kumyk
Kurd
Lak
Lebanese
Lebanese_Christian
Lebanese_Muslim
Lezgin
Lithuanian
Maltese
Moldavian
Mordovian
Norwegian
Orcadian
Ossetian
Palestinian
Romanian
Russian
Sardinian
Saudi
Scottish
Sicilian
Spanish
Spanish_North
Syrian
Tabasaran
Turkish
Ukrainian
```

The list can be downloaded [here](https://raw.githubusercontent.com/nevrome/mobest.analysis.2022/master/code/01_data_preparation/modern_western_eurasian_populations.txt).

:::

`trident` allows to download packages that contain samples with this group attribution from the community archive.

```{bash}
trident fetch -d refData --fetchFile modern_western_eurasian_populations.txt
```

```
refData
├── 2012_PattersonGenetics-2.1.3
├── 2014_LazaridisNature-4.0.2
├── 2016_LazaridisNature-2.1.3
├── 2019_Biagini_Spain-2.2.1
└── 2019_Jeong_InnerEurasia-3.0.1
```

This works, because the population list above already has the same structure as the selection language trident uses for `fetch` and `forge` (more about that below).

## Merging the reference data with our data

We now have our own Poseidon package for the ice mummy in a directory `ice` and various packages with modern reference data in `refData`. Our package features genotype data in EIGENSTRAT format, the reference data packages include data in PLINKs binary format.

`trident`s most notable feature is the ability to merge the exact samples we require from both data sources at once with the [`forge` subcommand](https://www.poseidon-adna.org/#/trident?id=forge-command).

```{bash}
trident forge \
  -d refData \
  -d ice \
  --forgeFile modern_western_eurasian_populations.txt \
  -f "<ice>" \
  --outFormat EIGENSTRAT \
  --intersect \
  -o iceWithRef
```

Note how we list two directories as data sources with `-d`/`--baseDir`. `forge` discovers and reads all Poseidon packages both in `refData` and `ice`.

It then constructs a subset- and merge operation, by parsing the selection language from both the `--forgeFile` `modern_western_eurasian_populations.txt` and the individual forge string `-f` `"<ice>"`. The forge language is a powerful DSL (domain specific language) that allows positive and negative selection of individuals (in `<...>`), populations and Poseidon packages (in `*...*`).

`forge` reads both data in EIGENSTRAT and in PLINK's binary format in constant memory via stream-processing. It can also produce both formats for the output package, though the default is the more space efficient PLINK format, so we have to set `--outFormat EIGENSTRAT` here for our downstream applications.

The `--intersect` flag is a reaction to the fact that our input packages feature different SNP sets: Some include only the ~600.000k HumanOrigins SNPs, others the ~1.240.000k 1240K SNPs. As a default `forge` returns the union of all SNP sets it encounters, which would here yield many missing entries for the modern reference data.

```{bash}
trident summarize -d iceWithRef
```

## Reducing the data size

```{bash}
qjanno "SELECT * FROM d(myDataWithRef)" -c

qjanno "
SELECT Poseidon_ID, Group_Name, row_number() OVER (PARTITION BY Group_Name ORDER BY random()) rn
FROM d(myDataWithRef)
" --raw > huhu.txt

qjanno "
SELECT '<'||Poseidon_ID||'>'
FROM huhu.txt
WHERE rn <= 2
ORDER BY Group_Name DESC;
" --raw --noOutHeader | wc -l
```

```{bash}
trident summarise -d myPac
```

```{bash}
trident survey -d myPac
```

```{bash}
#trident genoconvert -d myPac --outFormat EIGENSTRAT
```

.bib file

```{r}
ind <- readr::read_tsv(
  "scratch/myDataWithRef/myDataWithRef.ind",
  col_names = c("id", "sex", "pop")
)

pca_out <- smartsnp::smart_pca(
  "scratch/myDataWithRef/myDataWithRef.geno",
  sample_group = seq_len(nrow(ind)),
  missing_impute = "mean",
  sample_project = which(ind$id == "Iceman.SG"),
  pc_axes = 2
)

saveRDS(pca_out, file = "scratch/pca_out.rds")
```

```{r}
pca_out <- readRDS("scratch/pca_out.rds")

library(magrittr)
library(ggplot2)

pca_out$pca.sample_coordinates %>%
  ggplot() +
  geom_point(aes(x = PC1, y = PC2, color = Class))
```
